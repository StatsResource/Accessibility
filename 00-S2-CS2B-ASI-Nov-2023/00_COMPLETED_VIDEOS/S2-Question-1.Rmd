---
title: "Statistics with R"
subtitle: "Statistical Modelling with R for Actuarial Students"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(magrittr)
library(knitr)
library(dplyr)
library(janitor)


```

```{css, echo=FALSE}
pre {
  background: #ADD8E6;
  max-width: 100%;
  overflow-x: scroll;
}
```

***CS2B – Risk Modelling and Survival Analysis ***


* The emphasis is placed on being able to apply statistical methods to actuarial problems using real data sets and the open-source software environment R. 

* Time Series. Probability Distributions, Survival Analysis

---

### Time Series Analysis

* This exercise involves the comprehensive analysis and modeling of a time series using various statistical techniques. 

* Starting with the simulation of a time series, it proceeds to analyze its general features and plot autocorrelation and partial autocorrelation functions. 

* The exercise includes fitting various models, such as ***AR(1)***, ***AR(3)***, and ***ARMA(1,1)***, and selecting the best-fit model based on criteria like AIC. 

* It also involves predicting future values and validating the model's fit using residual analysis and the ***Ljung-Box Portmanteau test***.

* Overall, this exercise provides a detailed exploration of time series characteristics, modeling, and validation, allowing for a thorough understanding of the underlying data and appropriate forecasting methods.

---

**A.** Simulate a time series of length 500 observations in a line chart having following arguments:
<pre><code>
Seed Value = 100
ar = 0.9
ma = 0.2
order = c(1,1,1)
</code></pre>

**B.** Comment on the general features of the chart.

**C.** Plot the sample Autocorrelation Function (ACF) and sample Partial Autocorrelation function (PACF) of the data.

**D.** Determine the best least squares linear fit, adding it to your chart in ***Exercise A*** and paste the new chart.

**E.** Explain whether this least square linear trend can be removed such that stationary distribution is appropriate for the residuals.

**F.** Fit an AR(1), AR(3) and ARMA(1,1) model to the time series.

---

**G.** Determine the corresponding 95% Confidence Interval for the AR(1) model fitted above.

**H.** Mention the best fit model from your observations in ***Exercise F***. Calculate the predicted values using the best model fit above for 10 steps ahead.

**I.** Construct the Autocorrelation Function (ACF) and sample Partial Autocorrelation function (PACF) for the residuals of the best fitted model above and plot the graph.

**J.** Comment on the graphical outputs of ***Exercise C*** and ***Exercise I***.

**K.** Perform the Ljung and Box Portmanteau test for the residuals of the model with lag of 4, 6, 12 and comment whether the model is an appropriate fit. (use "lag = 4" etc and set <tt>"fitdf = 3"</tt>)


---

### Exercise A.

**Simulate a Time Series, using the given parameters**

```{r eval=FALSE}
set.seed(100) 
observations <- arima.sim(
  list(order = c(1,1,1),
       ar = 0.9, 
       ma = 0.2), 
  n = 500)

plot(observations, 
    main = "Line chart of time series observation")

```

---

### Exercise A.

**Simulate a Time Series, using the given parameters**

```{r echo=FALSE}
set.seed(100) 
observations <- arima.sim(
  list(order = c(1,1,1),
       ar = 0.9, 
       ma = 0.2), 
  n = 500)

plot(observations, 
    main = "Line chart of time series observation")

```


---

### Exercise B.

Based on the simulated data:

- The data is **not stationary** since the values change over time.

- There is a **downward trend** observed in the beginning and an **upward trend** towards the end, indicating non-stationarity.

- The **mean** and **standard deviation** are different at different points in time, which shows that the mean is not constant.


---

### Exercise C.

**Plot the sample Autocorrelation Function (ACF) and sample Partial Autocorrelation Function (PACF)**


```{r eval = FALSE}
acf(observations, main = "ACF")

pacf(observations, main = "PACF")
```


---

### Exercise C.

**Sample Autocorrelation Function (ACF)**


```{r echo=FALSE}
acf(observations, main = "ACF")
```


---

### Exercise C.

**Sample Partial Autocorrelation Function (PACF)**


```{r echo=FALSE}
pacf(observations, main = "PACF")
```

---

### Exercise D.

To determine and add the best least squares linear fit to the chart:

```{r}
x <- 1:length(observations)
leastsquarefit <- lm(observations ~ x)

coefficients <- coef(leastsquarefit)
coefficients 
```

---

### Exercise D.

To determine and add the best least squares linear fit to the chart:

```{r eval=FALSE}
plot(observations, 
     main = "Line Chart of Time Series Observations with Linear Fit")

abline(leastsquarefit, col = "red")
```
---

### Exercise D.


```{r echo=FALSE}
plot(observations, main = "Line Chart of Time Series Observations with Linear Fit")
abline(leastsquarefit, col = "red")
```



---

### Exercise  E.


```{r eval= FALSE}
plot(leastsquarefit$res, 
     xlab = "Time" , 
     ylab = "Residuals",
     pch=16,col="blue"
     )

abline(h=0,col="red",lty=2)
```

---

### Exercise  E.


```{r echo= FALSE}
plot(leastsquarefit$res, xlab = "Time" , ylab = "Residuals",
     pch=16,col="blue")

abline(h=0,col="red",lty=2)
```


---

### Exercise  E.

* It is clear that residuals are not stationary as they are negative in the first, then followed by positive residuals in the middle part and then negative in the last part.

---
### Exercise  E.

**Alternate Solution**

```{r fig.height=5}
acf(leastsquarefit$res)
```

The residuals are not stationary as the ACF values are decaying very slowly.

---

### Exercise  F.
 

```{r}
fit1 = arima(observations, order= c(1,0,0)) 
fit1 
```



---

### Exercise  F.

```{r}

fit2 = arima(observations, order= c(3,0,0)) 
fit2 

```


---

### Exercise  F.

```{r}
fit3 = arima(observations, order= c(1,0,1)) 
fit3 
```


---


### Exercise  F.

```{r}
AIC(fit1)
```

```{r}
AIC(fit2)
```

```{r}
AIC(fit3)
```

---


### Exercise G.

**Determine the corresponding 95% Confidence Interval for the AR(1) model**

* Quantiles (i.e. -1.96 and 1.96)
```{r}
Zs <- c(-1,1)*qnorm(0.975)
Zs
```

The variance-covariance matrix of the estimated coefficients from a fitted model object
```{r}
fit1$var.coef
```

---


### Exercise G.

**Determine the corresponding 95% Confidence Interval for the AR(1) model**


```{r}
fit1$coef[1] + (Zs * sqrt(fit1$var.coef[1,1]) )
```

The confidence interval is ( 0.998827 , 1.000529 )

---

### Exercise H.

The AIC is lowest for AR(3) (i.e. <tt>fit2</tt>) among the models above and hence is the best fit among the above models. 

```{r}
predict(fit2, n.ahead = 10) $pred
```

---

### Exercise I.
 
**ACF and PACF for Residuals of Best Fitted Model**

```{r eval=FALSE,fig.width=11}
par(mfrow = c(1,2)) 
acf(fit2$residuals) 
pacf(fit2$residuals)
```

---

### Exercise I.
 
**ACF and PACF for Residuals of Best Fitted Model**

```{r echo=FALSE,fig.width=11}
par(mfrow = c(1,2)) 
acf(fit2$residuals) 
pacf(fit2$residuals)
```

---

### Exercise J.

* PACF for the data shows no significance from lag 2 which could indicate stationarity but the ACF is decaying very slowly indicating it is not stationary. 

* For example, this is consistent with ARIMA (1,1,1) behaviour.

* The plot for the residuals generally lies within the confidence intervals. 

* This is consistent with the residuals forming the white noise process.


---

###  Ljung-Box Portmanteau Test 

The Ljung-Box Portmanteau test is used to determine whether the residuals from a time series model are independently distributed. In simpler terms, it checks if there is any autocorrelation (correlation over time) in the residuals of your model. 

Here’s what the test does:
- **Null Hypothesis (H0):** The residuals are independently distributed (no autocorrelation).
- **Alternative Hypothesis (H1):** The residuals are not independently distributed (autocorrelation present).

If the p-value from the test is low (typically less than 0.05), it suggests that there is significant autocorrelation in the residuals, meaning your model might not be capturing all the patterns in the data.

It's a useful diagnostic tool to assess the adequacy of a fitted time series model. 

If autocorrelation is detected, you may need to reconsider your model or include additional terms to capture the underlying structure of the data.

---
### Exercise K.

**Ljung-Box Portmanteau Test**

```{r eval = FALSE}
Box.test(fit2$residuals, type = "Ljung", fitdf = 3, lag = 4)
Box.test(fit2$residuals, type = "Ljung", fitdf = 3, lag = 6)
Box.test(fit2$residuals, type = "Ljung", fitdf = 3, lag = 12)
```


- **Results**: If the p-values are less than 0.05, it suggests that residuals are not white noise and further model refinement may be needed.


---

### Ljung-Box Test 1

```{r}
Box.test(fit2$residuals, type = "Ljung", fitdf = 3, lag = 4) 
```

---

### Ljung-Box Test 2


```{r}
Box.test(fit2$residuals, type = "Ljung", fitdf = 3, lag = 6) 

```

---

### Ljung-Box Test 3

```{r}
Box.test(fit2$residuals, type = "Ljung", fitdf = 3,lag = 12) 

```

---

### Ljung-Box Test

* The result above suggests that the residuals are forming a white noise process suggesting a good fit for <tt>ARIMA(3,0,0)</tt> model,

* The result above also suggests that the model requires differencing as it is consistent with <tt>ARIMA(p,1,q)</tt> behaviour.

* However, the three tests are not consistent with an <tt>ARIMA(3,0,0)</tt> model at the 5% significance level, since the p-values are lesser than 0.05

* Thus, there is not enough evidence to conclude <tt>ARIMA(3,0,0)</tt> to be a good fit.

* Also, we would expect the <tt>ARIMA(1,1,1)</tt> model that was used to generate the data to satisfy this test as well and thus can be shown to be also a good fit.
---



---

